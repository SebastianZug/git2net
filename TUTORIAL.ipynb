{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `git2net` - Extracting and analysing co-editing relationships from *git* repositories\n",
    "\n",
    "In this tutorial you will learn the basic steps required to obtain a co-editing relationship from a git repoitory using `git2net`.\n",
    "\n",
    "The tutorial assumes you have `git2net` installed. In addition, it is recommended to create a folder for this tutorial as additional files will be downloaded to your local directory (if not specified otherwise).\n",
    "\n",
    "```\n",
    " +---------------+   git2net          +---------------+\n",
    " | Repository on | -----------------> | local         |\n",
    " | Github        | clone_repository() | repository    |\n",
    " +---------------+   (1)              +---------------+\n",
    "                                        | git2net \n",
    "                                        | mine_git_repo()\n",
    "                                        v (2)\n",
    " +---------------+   pandas           +---------------+\n",
    " | pandas        |<------------------ | sqlite data   |\n",
    " | dataframe     |  read_sql_query()  | base          |\n",
    " +---------------+                    +---------------+ \n",
    "      |                                       |\n",
    "      v                                       v\n",
    " Data mining (4)                      Network analysis (3)\n",
    "```\n",
    "\n",
    "The following sections are structured according to this process model:\n",
    "\n",
    "1. Repository Cloning\n",
    "2. Repository Mining \n",
    "3. Network Analysis and visualization (git2net)\n",
    "4. Data analysis (pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository Cloning\n",
    "\n",
    "To start, you will need to select and clone a git repository that you are interested in analysing. For the purpose of this tutorial, we will analyse the repository behind `git2net`&mdash;aiming to finally find a solution to the well-known chicken and egg problem.\n",
    "\n",
    "The following lines will clone the `git2net` repository to your current working directory. To change this location, you can edit the path to the local directory stored in `local_directory`. The folder name of the repository is the name of the repository, which we store in `repo_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:09.784478Z",
     "start_time": "2020-04-22T14:05:08.464460Z"
    }
   },
   "outputs": [],
   "source": [
    "import pygit2 as git2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "git_repo_url = 'https://github.com/gotec/git2net.git'\n",
    "local_directory = '.'\n",
    "git_repo_dir = 'git2net4analysis'\n",
    "\n",
    "if os.path.exists(git_repo_dir):\n",
    "    shutil.rmtree(git_repo_dir)\n",
    "\n",
    "repo = git2.clone_repository(git_repo_url, git_repo_dir) # Clones a non-bare repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private repositories require some more efforts. Firstly, you have to generate a personal token. The procedure on GitHub side is explained [here](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token). Make sure to copy your new access token to a file (`secret.txt` for instance). You wonâ€™t be able to see it again! Please add `secret.txt` directly to your `.gitignore` file! You wouldn't believe how many access tokens are freely available at github :-)\n",
    "\n",
    "Now we are able to pass the token as a third parameter embedded in a callback method to `clone_repository()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygit2 as git2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "git_repo_url = 'https://github.com/user/SecretRepository.git'   # does not exist :-)\n",
    "local_directory = '.'\n",
    "git_repo_dir = 'secretRepository'\n",
    "\n",
    "f = open(\"secret.txt\", \"r\")\n",
    "token = f.read()\n",
    "\n",
    "if os.path.exists(git_repo_dir):\n",
    "    shutil.rmtree(git_repo_dir)\n",
    "\n",
    "callbacks = git2.RemoteCallbacks(git2.UserPass(token, 'x-oauth-basic'))\n",
    "repo = git2.clone_repository(git_repo_url, git_repo_dir, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository Mining\n",
    "\n",
    "Now that we have obtained a local copy of the repository, we can use `git2net` to obtain a database containing information on all commits and edits made to obtain the current state of the repository.\n",
    "\n",
    "To do so, we use the `mine_git_repo` function. This function takes two required inputs as well as a number of optional inputs, some of which we will further explore later in this tutorial. Let's start with the required inputs. Here, we need to supply a path to the git repositoy that will be analysed. Below, this is done with the variable `repo_name`. In addition, `git2net` requires a path to the *sqlite* database that will be filled during the mining process. This path is provided as `sqlite_db_file`.\n",
    "\n",
    "Note, that if no database exists on the supplied path, `git2net` will create a new database. If a database exists, `git2net` will check if the database was mined with the same setting and on the same repository and subsequently resume the mining process from wherever it was left off.\n",
    "\n",
    "Let's try this out. Below we import `git2net` and point it to the path to which we cloned the database. In addition, we specify the location of the database file in which the results of the mining process will be stored and ensure the database does currently not exist. We then run the `mine_git_repo` function with the optional argument `max_modifications = 1`. With this only commits in which 1 or less files were modified are mined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:18.221348Z",
     "start_time": "2020-04-22T14:05:09.787605Z"
    }
   },
   "outputs": [],
   "source": [
    "import git2net\n",
    "\n",
    "sqlite_db_file = 'git2net.db'\n",
    "\n",
    "# Remove database if exists\n",
    "if os.path.exists(sqlite_db_file):\n",
    "    os.remove(sqlite_db_file)\n",
    "\n",
    "max_modifications = 1\n",
    "    \n",
    "git2net.mine_git_repo(git_repo_dir, sqlite_db_file, max_modifications=max_modifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While mining, `git2net` provides information about the current progress. The first line shows that no database was found at the current path and mining will be started from scratch. This is totally expected, as we deliberately deleted any existing database before the run.\n",
    "\n",
    "Subsequently, progress updates on the mining process are printed. The first information denotes the number of processes `git2net` spawns and runs on. `git2net` is highly parallelised and will automatically detect the number of threads of your CPU, fully utilising all of them during operation. In case you want to reduce this load, this can be done by specifically setting the number of processes with the `no_of_processes` option of the `mine_git_repo` function.\n",
    "\n",
    "The other output shows the number of commits and total number of commits mined in this run, as well as the elapsed time and an estimate of the remaining time to finish.\n",
    "\n",
    "If a commit is skipped, the reason and the commit hash are printed. Currently, there are three cases in which a commit can be skipped. Firstly, as seen above a commit can exceed the maximum number of modifications set by `max_modifications`. Secondly, processing the commit can take longer as a maximum time defined by the `timeout` option. Thirdly, a commit can be skipped due to an error occuring within the commit. In these cases, please report the repository and commit hash in a new issue on github.com/gotec/git2net.\n",
    "\n",
    "Let's resume the mining process while increasing the maximum number of modifications to 5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:20.080736Z",
     "start_time": "2020-04-22T14:05:18.223206Z"
    }
   },
   "outputs": [],
   "source": [
    "max_modifications = 5\n",
    "\n",
    "git2net.mine_git_repo(git_repo_dir, sqlite_db_file, max_modifications=max_modifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output above, the process was resumed from the old database, skipping the already processed commits in the repository.\n",
    "\n",
    "Great, we made some progress and a large amount of the commits in the repository are already mined and in the database! But what about the other ones? We get some more information on the commits missing from the database from the `mining_state_summary` function. Similar to `mine_git_repo`, it also requires the paths to the repository as well as the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:20.320147Z",
     "start_time": "2020-04-22T14:05:20.082245Z"
    }
   },
   "outputs": [],
   "source": [
    "git2net.mining_state_summary(git_repo_dir, sqlite_db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function again provides a summary of the mining state, as well as details on all missing commits. Let's assume, we are very interest in commit *090c00c342283134a23900f85c1d232499617365* but want to avoid crawling the other missing commits. While this is uneccessary for small repositories such as `git2net` this might become higly relevant for larger projects such as `linux`, where individual commits can make changes to thousands of files which in turn require significant computational resources to analyse. This is particularly important for merge commits, as all files included in the diffs to both parent commits need to be considered. Therefore, for larger projects I generally recommend to run `git2net` with `max_modifications = 1000`, subsequently increasing this number if required.\n",
    "\n",
    "But now back to mining specifically commit *090c00c342283134a23900f85c1d232499617365*, which can be done with the `commits` option in `mine_git_repo`. We also set the number of processes to 1, enabling serial mode, which can be very helpful for debugging as significantly more information is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:20.875161Z",
     "start_time": "2020-04-22T14:05:20.322528Z"
    }
   },
   "outputs": [],
   "source": [
    "# mine_git_repo takes list of commits\n",
    "commits = ['090c00c342283134a23900f85c1d232499617365']\n",
    "\n",
    "git2net.mine_git_repo(git_repo_dir, sqlite_db_file, commits=commits, no_of_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have now mined your first git repository using `git2net`! Note, though that not all commits have been mined at this point. This will be done at a later stage of this tutorial.\n",
    "\n",
    "## Network Analysis and visualization\n",
    "\n",
    "You can now use the database to query various information on different commits or edits. In addition, `git2net` also provides the functionality to generate various network projections of the data.\n",
    "\n",
    "To start, lets try to obtain a co-editing network for our project. This is as simple as calling the `get_coediting_network` function and providing the database we just mined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:21.394450Z",
     "start_time": "2020-04-22T14:05:20.877538Z"
    }
   },
   "outputs": [],
   "source": [
    "t, node_info, edge_info = git2net.get_coediting_network(sqlite_db_file)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a `pathpy` temporal network object as well as two dictionaries which can be used to return properties of nodes and edges. As of writing this tutorial not all of them are used but they are set as placeholders for future versions of `git2net`.\n",
    "\n",
    "A `pathpy` temporal network object can be visualised by itself as shown above. In addition, we can also aggregate the network, by dropping the order of events, yielding a standard network object. Let's do this next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:21.402393Z",
     "start_time": "2020-04-22T14:05:21.396584Z"
    }
   },
   "outputs": [],
   "source": [
    "import pathpy as pp\n",
    "pp.Network.from_temporal_network(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both the temporal and aggregated network, a node represents an author, whereas edges point from the person changing a line of code to the person who was the original author.\n",
    "\n",
    "Next, we could ask the question which those files were that authors collaborated on. Therefore, we can plot a bipartite network containing both files and authors as nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:21.478314Z",
     "start_time": "2020-04-22T14:05:21.404409Z"
    }
   },
   "outputs": [],
   "source": [
    "t, node_info, edge_info = git2net.get_bipartite_network(sqlite_db_file)\n",
    "n = pp.Network.from_temporal_network(t)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this network, `node_info` contains the classes of authors in the network. These can e.g. be used to color nodes as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:21.484841Z",
     "start_time": "2020-04-22T14:05:21.479468Z"
    }
   },
   "outputs": [],
   "source": [
    "colour_map = {'author': '#73D2DE', 'file': '#2E5EAA'}\n",
    "node_color = {node: colour_map[node_info['class'][node]] for node in n.nodes}\n",
    "pp.visualisation.plot(n, node_color=node_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection of this network that links authors editing the same file is the co-authorship network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:21.985852Z",
     "start_time": "2020-04-22T14:05:21.485859Z"
    }
   },
   "outputs": [],
   "source": [
    "n, node_info, edge_info = git2net.get_coauthorship_network(sqlite_db_file)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it looks similar, however, all information on the direction of interactions is lost.\n",
    "\n",
    "If we are interested in e.g more recently edited files, we can filter the database by providing the `time_from` and `time_to` options. Let's check the files edited since May 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:22.048608Z",
     "start_time": "2020-04-22T14:05:21.987119Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "time_from = datetime(2019, 5, 1)\n",
    "t, node_info, edge_info = git2net.get_bipartite_network(sqlite_db_file, time_from=time_from)\n",
    "n = pp.Network.from_temporal_network(t)\n",
    "colour_map = {'author': '#73D2DE', 'file': '#2E5EAA'}\n",
    "node_color = {node: colour_map[node_info['class'][node]] for node in n.nodes}\n",
    "pp.visualisation.plot(n, node_color=node_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`git2net` allows the extraction of editing paths on the level of individual lines. I.e. we are able to track consecutive changes made to a single line over time&mdash;even if these lines move up or down in a file, or even across files. This is very powerful, as it allows us to determine editing sequences as well as find lines that require more editing than others. These could either be very difficult lines to implement or contain very important information, such as the version number in an `__init__.py` file.\n",
    "\n",
    "To extract these paths, we can use the `get_line_editing_paths` function. As these networks tend be very large we limit the analysis to a very small file for this tutorial. To only look at a specific set of file paths we can use the `file_paths` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:22.284128Z",
     "start_time": "2020-04-22T14:05:22.049839Z"
    }
   },
   "outputs": [],
   "source": [
    "file_paths = ['setup.py']\n",
    "paths, dag, node_info, edge_info = git2net.get_line_editing_paths(sqlite_db_file, git_repo_dir,\n",
    "                                                                  file_paths=file_paths)\n",
    "pp.visualisation.plot(dag, node_color=node_info['colors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that despite only looking at a single file the network shown above is not connected. This is due to our database not being complete. Let's fix this now and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:30.005350Z",
     "start_time": "2020-04-22T14:05:22.285815Z"
    }
   },
   "outputs": [],
   "source": [
    "git2net.mine_git_repo(git_repo_dir, sqlite_db_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T14:05:30.392380Z",
     "start_time": "2020-04-22T14:05:30.008446Z"
    }
   },
   "outputs": [],
   "source": [
    "paths, dag, node_info, edge_info = git2net.get_line_editing_paths(sqlite_db_file, git_repo_dir,\n",
    "                                                                  file_paths=file_paths)\n",
    "pp.visualisation.plot(dag, node_color=node_info['colors'], width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, these networks get very large very quickly. Therefore, it is often more useful to work with the `pathpy` path object that is also returned by the function. It cointains all paths and subpaths contained in the network shown above. More information regarding this object can be found in the documentation on [pathpy.net](http://www.pathpy.net/).\n",
    "\n",
    "This concludes this tutorial, which I hope you found useful. Enjoy using `git2net` and best of luck for your research! If you find any bugs with the code please let me know on [github.com](https://github.com/gotec/git2net).\n",
    "\n",
    "`git2net` has been developed as open source project. This means your ideas and inputs are highly welcome. Feel free to share the project and contribute yourself. You can imediately get started on the repository you just downloaded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a view on data base content. Obviously, we found two tables. The second code snippet illustrates the usage of sql statements to evaluate the content. We extract the unique author names from the `commit` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db = sqlite3.connect(sqlite_db_file)\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT DISTINCT author_name FROM commits\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a view on an alternative approach using python pandas. We load the data base in a pandas DataFrame in a first step. Afterwards the evaluation is much more comfortable due to the fact that the pandas API includes a huge amount of tools for statistical analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "db = sqlite3.connect(sqlite_db_file)\n",
    "pdCommits = pd.read_sql_query(\"SELECT * FROM commits\", db)\n",
    "\n",
    "pdCommits.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headline lists all extracted features contained in `pdCommits`. How many records/commits are in the history?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdCommits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number reflects the number of records, the second how man columns/categories are involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdCommits.author_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many commits did the individual authors contribute to the overall project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdCommits.groupby('author_name')['hash'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's easy. We want to give a last example that applies the features of pandas. At which time git2net commits are usually submitted? Of course, GitHub and GitLab offers similar visualizations, but based on git2net and pandas you are able to filter your data set individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pdCommits['timestamp'] = pd.to_datetime(pdCommits['author_date'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "pdCommits['hours'] = pdCommits.timestamp.dt.hour\n",
    "pdCommits['days'] = pdCommits.timestamp.dt.dayofweek\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "sns.heatmap(pdCommits.groupby(['days', 'hours'])['timestamp'].count().unstack(), annot=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram shows the week with Monday (0) to Sunday (6). Someone seams to work at the weekend too :-) Find out who it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
